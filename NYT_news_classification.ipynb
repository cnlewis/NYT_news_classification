{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'install_mathjax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f548963a1c15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexternal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmathjax\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmathjax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minstall_mathjax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'install_mathjax'"
     ]
    }
   ],
   "source": [
    "from IPython.external import mathjax;\n",
    "mathjax.install_mathjax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project: Applying Naive Bayes to text classification\n",
    "\n",
    "<center><img src=\"files/The_New_York_Times_logo.png\"></center>\n",
    "Automatically categorize news according to their title into twenty-eight standard topics. Looking at every New York Times front page story from 1996 to 2006, coded according to the Policy Agendas (http://www.policyagendas.org). This collection of data has been compiled by Amber E. Boydstun.\n",
    "\n",
    "Specifically interested in classifying news from The New York Times in the following macro-topics according to its title:\n",
    "\n",
    "\n",
    "\n",
    "<table border=\"1\">\n",
    "<tr>\n",
    "<td>\n",
    "1 \n",
    "<td>\n",
    "Macroeconomics\n",
    "<tr>\n",
    "<td>\n",
    "2 \n",
    "<td>\n",
    "Civil Rights, Minority Issues, and Civil Liberties \n",
    "<tr>\n",
    "<td>\n",
    "3\n",
    "<td>\n",
    "Health\n",
    "<tr>\n",
    "<td>\n",
    "4 \n",
    "<td>Agriculture\n",
    "<tr>\n",
    "<td>\n",
    "5 \n",
    "<td>Labor, Employment, and Immigration\n",
    "<tr>\n",
    "<td>\n",
    "6 \n",
    "<td> Education\n",
    "<tr>\n",
    "<td>\n",
    "7\n",
    "<td>Environment\n",
    "<tr>\n",
    "<td>\n",
    "8\n",
    "<td>Energy\n",
    "<tr>\n",
    "<td>\n",
    "10 \n",
    "<td>Transportation\n",
    "<tr>\n",
    "<td>\n",
    "12 \n",
    "<td>Law, Crime, and Family Issues\n",
    "<tr>\n",
    "<td>\n",
    "13 \n",
    "<td>Social Welfare\n",
    "<tr>\n",
    "<td>\n",
    "14 \n",
    "<td>Community Development and Housing Issues\n",
    "<tr>\n",
    "<td>\n",
    "15 \n",
    "<td>Banking, Finance, and Domestic Commerce\n",
    "<tr>\n",
    "<td>\n",
    "16 \n",
    "<td>Defense\n",
    "<tr>\n",
    "<td>\n",
    "17 \n",
    "<td>Space, Science, Technology and Communications\n",
    "<tr>\n",
    "<td>\n",
    "18 \n",
    "<td>Foreign Trade\n",
    "<tr>\n",
    "<td>\n",
    "19 \n",
    "<td>International Affairs and Foreign Aid\n",
    "<tr>\n",
    "<td>\n",
    "20 \n",
    "<td>Government Operations\n",
    "<tr>\n",
    "<td>\n",
    "21 \n",
    "<td>Public Lands and Water Management\n",
    "<tr>\n",
    "<td>\n",
    "24 \n",
    "<td>State and Local Government Administration\n",
    "<tr>\n",
    "<td>\n",
    "26 \n",
    "<td>Weather and Natural Disasters\n",
    "<tr>\n",
    "<td>\n",
    "27 \n",
    "<td>Fires\n",
    "<tr>\n",
    "<td>\n",
    "28 \n",
    "<td>Arts and Entertainment\n",
    "<tr>\n",
    "<td>\n",
    "29 \n",
    "<td>Sports and Recreation\n",
    "<tr>\n",
    "<td>\n",
    "30 \n",
    "<td>Death Notices\n",
    "<tr>\n",
    "<td>\n",
    "31 \n",
    "<td>Churches and Religion\n",
    "<tr>\n",
    "<td>\n",
    "99 \n",
    "<td>Other, Miscellaneous, and Human Interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article_ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Article_Sequence</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Topic_6digit</th>\n",
       "      <th>Topic_4digit</th>\n",
       "      <th>Topic_2digit</th>\n",
       "      <th>War on Terror</th>\n",
       "      <th>Katrina</th>\n",
       "      <th>Israel/Palestine</th>\n",
       "      <th>Immigration</th>\n",
       "      <th>Presidential Elections</th>\n",
       "      <th>Clinton Impeachment</th>\n",
       "      <th>Enron</th>\n",
       "      <th>Darfur</th>\n",
       "      <th>Race/Ethnicity</th>\n",
       "      <th>Schiavo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1/1/1996</td>\n",
       "      <td>a</td>\n",
       "      <td>Nation's Smaller Jails Struggle To Cope With S...</td>\n",
       "      <td>Jails overwhelmed with hardened criminals</td>\n",
       "      <td>120500</td>\n",
       "      <td>1205</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1/1/1996</td>\n",
       "      <td>b</td>\n",
       "      <td>Dancing (and Kissing) In the New Year</td>\n",
       "      <td>new years activities</td>\n",
       "      <td>280000</td>\n",
       "      <td>2800</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1/1/1996</td>\n",
       "      <td>c</td>\n",
       "      <td>Forbes's Silver Bullet for the Nation's Malaise</td>\n",
       "      <td>Steve Forbes running for President</td>\n",
       "      <td>201201</td>\n",
       "      <td>2012</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1/1/1996</td>\n",
       "      <td>d</td>\n",
       "      <td>Up at Last, Bridge to Bosnia Is Swaying Gatewa...</td>\n",
       "      <td>U.S. military constructs bridge to help their ...</td>\n",
       "      <td>160200</td>\n",
       "      <td>1602</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1/1/1996</td>\n",
       "      <td>e</td>\n",
       "      <td>2 SIDES IN SENATE DISAGREE ON PLAN TO END FURL...</td>\n",
       "      <td>Democrats and Republicans can't agree on plan ...</td>\n",
       "      <td>201206</td>\n",
       "      <td>2012</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Article_ID      Date Article_Sequence  \\\n",
       "0           1  1/1/1996                a   \n",
       "1           2  1/1/1996                b   \n",
       "2           3  1/1/1996                c   \n",
       "3           4  1/1/1996                d   \n",
       "4           5  1/1/1996                e   \n",
       "\n",
       "                                               Title  \\\n",
       "0  Nation's Smaller Jails Struggle To Cope With S...   \n",
       "1             Dancing (and Kissing) In the New Year    \n",
       "2   Forbes's Silver Bullet for the Nation's Malaise    \n",
       "3  Up at Last, Bridge to Bosnia Is Swaying Gatewa...   \n",
       "4  2 SIDES IN SENATE DISAGREE ON PLAN TO END FURL...   \n",
       "\n",
       "                                             Summary  Topic_6digit  \\\n",
       "0          Jails overwhelmed with hardened criminals        120500   \n",
       "1                               new years activities        280000   \n",
       "2                 Steve Forbes running for President        201201   \n",
       "3  U.S. military constructs bridge to help their ...        160200   \n",
       "4  Democrats and Republicans can't agree on plan ...        201206   \n",
       "\n",
       "   Topic_4digit  Topic_2digit  War on Terror  Katrina  Israel/Palestine  \\\n",
       "0          1205            12              0        0                 0   \n",
       "1          2800            28              0        0                 0   \n",
       "2          2012            20              0        0                 0   \n",
       "3          1602            16              0        0                 0   \n",
       "4          2012            20              0        0                 0   \n",
       "\n",
       "   Immigration  Presidential Elections  Clinton Impeachment  Enron  Darfur  \\\n",
       "0            0                       0                    0      0       0   \n",
       "1            0                       0                    0      0       0   \n",
       "2            0                       1                    0      0       0   \n",
       "3            0                       0                    0      0       0   \n",
       "4            0                       0                    0      0       0   \n",
       "\n",
       "   Race/Ethnicity  Schiavo  \n",
       "0               0        0  \n",
       "1               0        0  \n",
       "2               0        0  \n",
       "3               0        0  \n",
       "4               0        0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f\n",
    "#load data\n",
    "import pandas as pd\n",
    "data=pd.read_csv('./files/Boydstun_NYT_FrontPage_Dataset_1996-2006_0.csv')\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data set in two set: \n",
    "    \n",
    "+ Train the classifier with news up to 2004.\n",
    "+ Test the classifier with news from 2005 and 2006."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check the split sizes, train, test and total amount of data:\n",
      "(23584L,) (7450L,) (31034L,)\n",
      "Display the labels:\n",
      "[ 1  2  3  4  5  6  7  8 10 12 13 14 15 16 17 18 19 20 21 24 26 27 28 29 30\n",
      " 31 99]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#Training the classifier with data up to 1/1/2004 and test its performnace in data from 2004-2006\n",
    "split = pd.to_datetime(pd.Series(data['Date']))<pd.datetime(2004, 1, 1)\n",
    "raw_data = data['Title']\n",
    "raw_train = raw_data[split]\n",
    "raw_test = raw_data[np.logical_not(split)]\n",
    "y = data['Topic_2digit']\n",
    "y_train = y[split]\n",
    "y_test = y[np.logical_not(split)]\n",
    "print 'Check the split sizes, train, test and total amount of data:'\n",
    "print raw_train.shape, raw_test.shape, raw_data.shape\n",
    "print 'Display the labels:'\n",
    "print np.unique(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Nation's Smaller Jails Struggle To Cope With Surge in Inmates \n",
      "\n",
      "Preprocessed: nation's smaller jails struggle to cope with surge in inmates \n",
      "\n",
      "Tokenized:[u'Nation', u'Smaller', u'Jails', u'Struggle', u'To', u'Cope', u'With', u'Surge', u'in', u'Inmates']\n",
      "\n",
      "Analyzed data string:[u'nation', u'smaller', u'jails', u'struggle', u'cope', u'surge', u'inmates']\n",
      "\n",
      "Number of tokens: 8950\n",
      "\n",
      "Extract of tokens:\n",
      "[u'boeing', u'boiling', u'boils', u'bold', u'bolster', u'bolsters', u'bolt', u'bolts', u'bomb', u'bombay', u'bombed', u'bomber', u'bombers', u'bombing', u'bombings', u'bombs', u'bonanza', u'bond', u'bondage', u'bonds', u'bone', u'bones', u'bonn', u'bono', u'bonus', u'bonuses', u'book', u'books', u'booksellers', u'bookstore', u'boom', u'boomers', u'booming', u'booms', u'boost', u'boot', u'bora', u'border', u'borders', u'born', u'borough', u'boroughs', u'borrow', u'borrowing', u'bosnia', u'bosnian', u'bosnians', u'boss', u'bosses', u'boston', u'botched', u'bottle', u'bought', u'bounce', u'bound', u'bounty', u'bout', u'bow', u'bowing', u'bowl', u'bows', u'box', u'boxes', u'boxing', u'boy', u'boycott', u'boys', u'brace', u'braced', u'braces', u'bracket', u'bradley', u'brain', u'brains', u'branches', u'brand', u'brash', u'brave', u'bravery', u'braves', u'brawl', u'brawley', u'brazil', u'brazilian', u'breach', u'breaches', u'bread', u'break', u'breakdown', u'breaking', u'breaks', u'breakthrough', u'breakup', u'breast', u'breather', u'breed', u'brewers', u'brewing', u'bribery', u'bricks']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Using the count number of instances considering that a word has a minimum support of two documents\n",
    "vectorizer = CountVectorizer(min_df=2,                             \n",
    " stop_words='english', \n",
    " strip_accents='unicode')\n",
    "\n",
    "\n",
    "test_string = unicode(raw_train[0])\n",
    "print \"Example: \" + test_string +\"\\n\"\n",
    "print \"Preprocessed: \" + vectorizer.build_preprocessor()(test_string)+\"\\n\"\n",
    "print \"Tokenized:\" + str(vectorizer.build_tokenizer()(test_string))+\"\\n\"\n",
    "print \"Analyzed data string:\" + str(vectorizer.build_analyzer()(test_string))+\"\\n\"\n",
    "\n",
    "\n",
    "#Process and convert data\n",
    "X_train = vectorizer.fit_transform(raw_train)\n",
    "X_test = vectorizer.transform(raw_test)\n",
    "\n",
    "print \"Number of tokens: \" + str(len(vectorizer.get_feature_names())) +\"\\n\"\n",
    "print \"Extract of tokens:\"\n",
    "print vectorizer.get_feature_names()[1000:1100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification accuracy: 0.434899328859\n",
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.32      0.64      0.43        56\n",
      "          2       0.01      0.67      0.01         3\n",
      "          3       0.51      0.65      0.57       343\n",
      "          4       0.00      0.00      0.00         0\n",
      "          5       0.01      1.00      0.01         1\n",
      "          6       0.13      0.96      0.23        27\n",
      "          7       0.00      0.00      0.00         0\n",
      "          8       0.00      0.00      0.00         0\n",
      "         10       0.00      0.00      0.00         0\n",
      "         12       0.46      0.43      0.44       466\n",
      "         13       0.00      0.00      0.00         0\n",
      "         14       0.00      0.00      0.00         0\n",
      "         15       0.09      0.54      0.16        57\n",
      "         16       0.54      0.57      0.55      1259\n",
      "         17       0.03      1.00      0.06         4\n",
      "         18       0.00      0.00      0.00         0\n",
      "         19       0.81      0.34      0.48      3544\n",
      "         20       0.75      0.45      0.57      1555\n",
      "         21       0.00      0.00      0.00         0\n",
      "         24       0.00      0.00      0.00         0\n",
      "         26       0.00      0.00      0.00         0\n",
      "         27       0.00      0.00      0.00         0\n",
      "         28       0.00      0.00      0.00         1\n",
      "         29       0.35      0.60      0.44       134\n",
      "         30       0.00      0.00      0.00         0\n",
      "         31       0.00      0.00      0.00         0\n",
      "         99       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.70      0.43      0.51      7450\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1076: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "nb = BernoulliNB()\n",
    "nb.fit(X_train,y_train)\n",
    "\n",
    "y_hat = nb.predict(X_test)\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_confusion_matrix(y_pred, y):\n",
    "    plt.imshow(metrics.confusion_matrix(y, y_pred), interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('true value')\n",
    "    plt.xlabel('predicted value')\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(9,9)    \n",
    "    \n",
    "print \"classification accuracy:\", metrics.accuracy_score(y_hat, y_test)\n",
    "plot_confusion_matrix(y_hat, y_test)\n",
    "print \"Classification Report:\"\n",
    "print metrics.classification_report(y_hat,np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "ofname = open('NYT_data.pkl', 'wb')\n",
    "s = pickle.dump([X_train,y_train,X_test,y_test],ofname)\n",
    "ofname.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code: 1 Terms : [u'cut', u'bush', u'economy', u'budget', u'tax']\n",
      "Code: 2 Terms : [u'race', u'gay', u'new', u'court', u'abortion']\n",
      "Code: 3 Terms : [u'care', u'medicare', u'drug', u'health', u'new']\n",
      "Code: 4 Terms : [u'farm', u'safety', u'new', u'farmers', u'food']\n",
      "Code: 5 Terms : [u'workers', u'strike', u'union', u'immigrants', u'new']\n",
      "Code: 6 Terms : [u'students', u'city', u'new', u'school', u'schools']\n",
      "Code: 7 Terms : [u'rules', u'warming', u'air', u'new', u'pollution']\n",
      "Code: 8 Terms : [u'blackout', u'california', u'power', u'energy', u'oil']\n",
      "Code: 10 Terms : [u'new', u'security', u'800', u'flight', u'crash']\n",
      "Code: 12 Terms : [u'drug', u'case', u'death', u'new', u'police']\n",
      "Code: 13 Terms : [u'plan', u'security', u'new', u'social', u'welfare']\n",
      "Code: 14 Terms : [u'city', u'homeless', u'york', u'rent', u'new']\n",
      "Code: 15 Terms : [u'new', u'billion', u'deal', u'enron', u'microsoft']\n",
      "Code: 16 Terms : [u'bush', u'challenged', u'war', u'iraq', u'nation']\n",
      "Code: 17 Terms : [u'space', u'nasa', u'loss', u'new', u'shuttle']\n",
      "Code: 18 Terms : [u'business', u'bush', u'clinton', u'china', u'trade']\n",
      "Code: 19 Terms : [u'mideast', u'war', u'israel', u'new', u'china']\n",
      "Code: 20 Terms : [u'2000', u'clinton', u'bush', u'president', u'campaign']\n",
      "Code: 21 Terms : [u'park', u'plan', u'zero', u'ground', u'new']\n",
      "Code: 24 Terms : [u'mayor', u'giuliani', u'city', u'budget', u'new']\n",
      "Code: 26 Terms : [u'blizzard', u'new', u'overview', u'hurricane', u'storm']\n",
      "Code: 27 Terms : [u'blaze', u'ferry', u'killed', u'crash', u'fires']\n",
      "Code: 28 Terms : [u'arts', u'tv', u'broadway', u'art', u'new']\n",
      "Code: 29 Terms : [u'world', u'playoffs', u'series', u'yankees', u'baseball']\n",
      "Code: 30 Terms : [u'87', u'79', u'crash', u'dead', u'dies']\n",
      "Code: 31 Terms : [u'faith', u'bishop', u'new', u'church', u'pope']\n",
      "Code: 99 Terms : [u'editors', u'today', u'readers', u'note', u'special']\n"
     ]
    }
   ],
   "source": [
    "#Top N most predictive features per class\n",
    "N = 5\n",
    "voc = vectorizer.get_feature_names()\n",
    "for i, label in enumerate(np.unique(y)):\n",
    "    topN = np.argsort(nb.coef_[i])[-N:]\n",
    "    print 'Code: '+ str(label) + ' Terms : '+ str([voc[i] for i in topN])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding to the data set with the summary of the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data = data['Title']+data['Summary']\n",
    "raw_train = raw_data[split]\n",
    "raw_test = raw_data[np.logical_not(split)]\n",
    "y = data['Topic_2digit']\n",
    "y_train = y[split]\n",
    "y_test = y[np.logical_not(split)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Nation's Smaller Jails Struggle To Cope With Surge in Inmates Jails overwhelmed with hardened criminals\n",
      "\n",
      "Preprocessed: nation's smaller jails struggle to cope with surge in inmates jails overwhelmed with hardened criminals\n",
      "\n",
      "Tokenized:[u'Nation', u'Smaller', u'Jails', u'Struggle', u'To', u'Cope', u'With', u'Surge', u'in', u'Inmates', u'Jails', u'overwhelmed', u'with', u'hardened', u'criminals']\n",
      "\n",
      "Analyzed data string:[u'nation', u'smaller', u'jails', u'struggle', u'cope', u'surge', u'inmates', u'jails', u'overwhelmed', u'hardened', u'criminals']\n",
      "\n",
      "\n",
      "\n",
      "Number of tokens: 11354\n",
      "\n",
      "Extract of tokes:\n",
      "[u'banned', u'banner', u'banning', u'bans', u'bansenate', u'banus', u'baptist', u'baptists', u'bar', u'barak', u'barbie', u'bare', u'barely', u'bares', u'bargain', u'bargaining', u'bargains', u'barnes', u'barney', u'baron', u'barons', u'barrage', u'barred', u'barrel', u'barren', u'barrier', u'barriers', u'barring', u'bars', u'barter', u'base', u'baseball', u'based', u'basement', u'bases', u'basespentagon', u'bashing', u'basic', u'basis', u'baskeball', u'basketball', u'basks', u'basra', u'bastion', u'bat', u'bath', u'batter', u'battered', u'battering', u'batters', u'battery', u'battle', u'battledemocrats', u'battlefield', u'battleground', u'battles', u'battleus', u'battling', u'bay', u'bayer', u'bbc', u'beach', u'beaches', u'bear', u'bearing', u'bears', u'beat', u'beaten', u'beating', u'beats', u'beautiful', u'beauty', u'beckons', u'bed', u'bedevil', u'bedeviled', u'beef', u'beetle', u'began', u'begin', u'beginning', u'begins', u'beginsphoto', u'begun', u'behavior', u'behemoth', u'beijing', u'beijingus', u'beirut', u'belarus', u'belated', u'belfast', u'belgrade', u'beliefs', u'belies', u'believe', u'believed', u'believes', u'belittles', u'bell']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=2, \n",
    " stop_words='english', \n",
    " strip_accents='unicode')\n",
    "\n",
    "#example\n",
    "test_string = unicode(raw_train[0])\n",
    "print \"Example: \" + test_string +\"\\n\"\n",
    "print \"Preprocessed: \" + vectorizer.build_preprocessor()(test_string)+\"\\n\"\n",
    "print \"Tokenized:\" + str(vectorizer.build_tokenizer()(test_string))+\"\\n\"\n",
    "print \"Analyzed data string:\" + str(vectorizer.build_analyzer()(test_string))+\"\\n\"\n",
    "\n",
    "\n",
    "#Fit and convert data\n",
    "X_train = vectorizer.fit_transform(raw_train)\n",
    "X_test = vectorizer.transform(raw_test)\n",
    "\n",
    "print \"\\n\"\n",
    "print \"Number of tokens: \" + str(len(vectorizer.get_feature_names())) +\"\\n\"\n",
    "print \"Extract of tokes:\"\n",
    "print vectorizer.get_feature_names()[1000:1100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification accuracy: 0.515570469799\n",
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.45      0.61      0.52        83\n",
      "          2       0.06      0.94      0.11        18\n",
      "          3       0.69      0.59      0.63       514\n",
      "          4       0.00      0.00      0.00         0\n",
      "          5       0.03      0.67      0.06         9\n",
      "          6       0.46      0.87      0.61       106\n",
      "          7       0.00      0.00      0.00         0\n",
      "          8       0.00      0.00      0.00         0\n",
      "         10       0.01      0.50      0.02         2\n",
      "         12       0.61      0.41      0.49       651\n",
      "         13       0.00      0.00      0.00         0\n",
      "         14       0.00      0.00      0.00         0\n",
      "         15       0.23      0.55      0.32       136\n",
      "         16       0.70      0.64      0.67      1466\n",
      "         17       0.06      1.00      0.11         8\n",
      "         18       0.00      0.00      0.00         0\n",
      "         19       0.77      0.41      0.53      2816\n",
      "         20       0.85      0.55      0.67      1436\n",
      "         21       0.00      0.00      0.00         0\n",
      "         24       0.07      0.85      0.13        13\n",
      "         26       0.00      0.00      0.00         0\n",
      "         27       0.00      0.00      0.00         0\n",
      "         28       0.02      0.57      0.04         7\n",
      "         29       0.54      0.67      0.60       185\n",
      "         30       0.00      0.00      0.00         0\n",
      "         31       0.00      0.00      0.00         0\n",
      "         99       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.72      0.52      0.58      7450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "nb = BernoulliNB()\n",
    "nb.fit(X_train,y_train)\n",
    "\n",
    "y_hat = nb.predict(X_test)\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_confusion_matrix(y_pred, y):\n",
    "    plt.imshow(metrics.confusion_matrix(y, y_pred), interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('true value')\n",
    "    plt.xlabel('predicted value')\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(9,9)    \n",
    "    \n",
    "print \"classification accuracy:\", metrics.accuracy_score(y_hat, y_test)\n",
    "plot_confusion_matrix(y_hat, y_test)\n",
    "print \"Classification Report:\"\n",
    "print metrics.classification_report(y_hat,np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "ofname = open('NYT_context_data.pkl', 'wb')\n",
    "s = pickle.dump([X_train,y_train,X_test,y_test],ofname)\n",
    "ofname.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code: 1 Terms : [u'cut', u'economy', u'market', u'budget', u'tax']\n",
      "Code: 2 Terms : [u'gay', u'race', u'new', u'court', u'abortion']\n",
      "Code: 3 Terms : [u'medicare', u'care', u'drug', u'new', u'health']\n",
      "Code: 4 Terms : [u'disease', u'farm', u'new', u'farmers', u'food']\n",
      "Code: 5 Terms : [u'new', u'workers', u'strike', u'union', u'immigrants']\n",
      "Code: 6 Terms : [u'education', u'students', u'new', u'schools', u'school']\n",
      "Code: 7 Terms : [u'water', u'pollution', u'new', u'global', u'warming']\n",
      "Code: 8 Terms : [u'gas', u'prices', u'energy', u'oil', u'power']\n",
      "Code: 10 Terms : [u'investigation', u'800', u'twa', u'flight', u'crash']\n",
      "Code: 12 Terms : [u'death', u'scandal', u'abuse', u'new', u'police']\n",
      "Code: 13 Terms : [u'security', u'clinton', u'social', u'new', u'welfare']\n",
      "Code: 14 Terms : [u'housing', u'york', u'rent', u'nyc', u'new']\n",
      "Code: 15 Terms : [u'new', u'merger', u'scandal', u'antitrust', u'microsoft']\n",
      "Code: 16 Terms : [u'bush', u'challenged', u'war', u'nation', u'iraq']\n",
      "Code: 17 Terms : [u'loss', u'columbia', u'space', u'shuttle', u'new']\n",
      "Code: 18 Terms : [u'deal', u'sanctions', u'clinton', u'china', u'trade']\n",
      "Code: 19 Terms : [u'war', u'peace', u'new', u'china', u'israel']\n",
      "Code: 20 Terms : [u'2000', u'bush', u'president', u'clinton', u'campaign']\n",
      "Code: 21 Terms : [u'memorial', u'zero', u'ground', u'indian', u'new']\n",
      "Code: 24 Terms : [u'city', u'budget', u'governor', u'mayor', u'new']\n",
      "Code: 26 Terms : [u'hurricane', u'york', u'storm', u'new', u'weather']\n",
      "Code: 27 Terms : [u'fires', u'killed', u'accident', u'crash', u'new']\n",
      "Code: 28 Terms : [u'york', u'day', u'museum', u'art', u'new']\n",
      "Code: 29 Terms : [u'world', u'playoffs', u'series', u'yankees', u'baseball']\n",
      "Code: 30 Terms : [u'plane', u'death', u'crash', u'dead', u'dies']\n",
      "Code: 31 Terms : [u'catholic', u'religious', u'new', u'pope', u'church']\n",
      "Code: 99 Terms : [u'park', u'editors', u'new', u'special', u'note']\n"
     ]
    }
   ],
   "source": [
    "#Top N most predictive features per class\n",
    "N = 5\n",
    "voc = vectorizer.get_feature_names()\n",
    "for i, label in enumerate(np.unique(y)):\n",
    "    topN = np.argsort(nb.coef_[i])[-N:]\n",
    "    print 'Code: '+ str(label) + ' Terms : '+ str([voc[i] for i in topN])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the small summary improves the recognition rate by $10\\%$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
